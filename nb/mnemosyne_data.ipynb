{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import pygraphviz as pgv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lentil import datatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)\n",
    "mpl.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Mnemosyne log data from [here](https://archive.org/details/20140127MnemosynelogsAll.db) and add the decompressed contents to the `data` directory. I would recommend creating a reasonably-sized random sample of logs from the full db before loading the data into this notebook, since there are ~120 million logs in total. You can use the following commands:\n",
    "\n",
    "```\n",
    "sqlite3 2014-01-27-mnemosynelogs-all.db\n",
    ".mode csv\n",
    ".headers on\n",
    ".output mnemosynelogs_mini.csv\n",
    "select * from log order by Random() limit 10000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data set manageably smaller by filtering out users with short/long review histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unfiltered_logs = pd.read_table(os.path.join('data', 'mnemosynelogs_mini.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_ixns_of_user = unfiltered_logs['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_ids = unfiltered_logs['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mn = 10\n",
    "mx = 2500\n",
    "len(user_ids), sum(1 for x in user_ids if num_ixns_of_user[x] > mn and num_ixns_of_user[x] < mx), sum(num_ixns_of_user[x] for x in user_ids if num_ixns_of_user[x] > mn and num_ixns_of_user[x] < mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_ids = {x for x in user_ids if num_ixns_of_user[x] > mn and num_ixns_of_user[x] < mx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_logs = unfiltered_logs[unfiltered_logs['user_id'].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_logs.to_csv(os.path.join('data', 'mnemosynelogs_mini_filtered.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the filtered logs and compute basic stats summarizing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'mnemosynelogs_mini_filtered.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print '\\n'.join(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df[~np.isnan(df['grade'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Number of interactions = %d\" % len(df)\n",
    "print \"Number of unique students = %d\" % len(df['user_id'].unique())\n",
    "print \"Number of unique modules = %d\" % len(df['object_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "av = np.array(df['actual_interval'].values)\n",
    "sv = np.array(df['scheduled_interval'].values)\n",
    "av, sv = zip(*[(x, y) for x, y in zip(av, sv) if x>0 and y>0 and not np.isnan(x) and not np.isnan(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "av = np.array(av)\n",
    "sv = np.array(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('log10(Scheduled interval) (log10-milliseconds)')\n",
    "plt.ylabel('Frequency (number of interactions)')\n",
    "plt.hist(np.log10(sv+1), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('log10(Scheduled interval) (log10-milliseconds)')\n",
    "plt.ylabel('log10(Actual interval) (log10-milliseconds)')\n",
    "plt.scatter(np.log10(sv+1), np.log10(av+1), alpha=0.005)\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'scheduled-vs-actual-intervals.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = np.array(df['user_id'].value_counts().values)\n",
    "\n",
    "plt.xlabel('log10(Number of interactions per student)')\n",
    "plt.ylabel('Frequency (number of students)')\n",
    "plt.hist(np.log10(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = np.array(df['object_id'].value_counts().values)\n",
    "\n",
    "plt.xlabel('log10(Number of interactions per problem)')\n",
    "plt.ylabel('Frequency (number of problems)')\n",
    "plt.hist(np.log10(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grades = np.array(df['grade'].values)\n",
    "\n",
    "plt.hist(grades[~np.isnan(grades)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply more filters and format the log data into an `InteractionHistory` that can be understood by [lentil](https://github.com/rddy/lentil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interaction_history_from_mnemosyne_data_set(data):\n",
    "    \"\"\"\n",
    "    Parse Mnemosyne data set into an interaction history\n",
    "    \n",
    "    :param pd.DataFrame data: A dataframe of raw log data\n",
    "    :rtype: datatools.InteractionHistory\n",
    "    :return: An interaction history object\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data[data['grade'].apply(lambda x: not np.isnan(x))]\n",
    "    \n",
    "    data = data[['user_id', 'student_id', 'object_id', 'grade', 'timestamp', 'thinking_time', 'actual_interval', 'scheduled_interval']]\n",
    "    data.columns = ['user_id', 'student_id', 'module_id', 'outcome', 'timestamp', 'duration', 'actual_interval', 'scheduled_interval']\n",
    "        \n",
    "    data['outcome'] = data['outcome'].apply(lambda x: x > 1)\n",
    "    \n",
    "    student_timesteps = defaultdict(int)\n",
    "    timesteps = [None] * len(data)\n",
    "    for i, (_, ixn) in enumerate(data.iterrows()):\n",
    "        student_timesteps[ixn['student_id']] += 1\n",
    "        timesteps[i] = student_timesteps[ixn['student_id']]\n",
    "    data['timestep'] = timesteps\n",
    "    \n",
    "    data['module_type'] = [datatools.AssessmentInteraction.MODULETYPE] * len(data)\n",
    "    \n",
    "    return datatools.InteractionHistory(data, sort_by_timestep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is helpful for splitting histories by user-item pair (instead of by user) in lentil.evaluate\n",
    "df['student_id'] = [str(x['user_id'])+'-'+str(x['object_id']) for _, x in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unfiltered_history = interaction_history_from_mnemosyne_data_set(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unfiltered_history.data['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply additional data filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_history(history, min_num_ixns=5, max_num_ixns=sys.maxint):\n",
    "    \"\"\"\n",
    "    Filter history for students with histories of bounded length,\n",
    "    and modules with enough interactions\n",
    "    \n",
    "    :param datatools.InteractionHistory history: An interaction history\n",
    "    :param int min_num_ixns: Minimum number of timesteps in student history,\n",
    "        and minimum number of interactions for module\n",
    "    \n",
    "    :param int max_num_ixns: Maximum number of timesteps in student history\n",
    "    :rtype: datatools.InteractionHistory\n",
    "    :return: A filtered interaction history\n",
    "    \"\"\"\n",
    "    students = set(history.data['student_id'][(\n",
    "                history.data['timestep'] > min_num_ixns) & (\n",
    "                history.data['module_type']==datatools.AssessmentInteraction.MODULETYPE)])\n",
    "    students -= set(history.data['student_id'][history.data['timestep'] >= max_num_ixns])\n",
    "    \n",
    "    modules = {module_id for module_id, group in history.data.groupby('module_id') if len(group) > min_num_ixns}\n",
    "\n",
    "    return datatools.InteractionHistory(\n",
    "        history.data[(history.data['student_id'].isin(students)) & (\n",
    "                history.data['module_id'].isin(modules))],\n",
    "        reindex_timesteps=True,\n",
    "        size_of_test_set=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply the filter a couple of times, since removing student histories\n",
    "# may cause certain modules to drop below the min_num_ixns threshold,\n",
    "# and removing modules may cause student histories to drop below\n",
    "# the min_num_ixns threshold\n",
    "REPEATED_FILTER = 3 # number of times to repeat filtering\n",
    "history = reduce(\n",
    "    lambda acc, _: filter_history(acc, min_num_ixns=3, max_num_ixns=10000), \n",
    "    range(REPEATED_FILTER), unfiltered_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to pickled interaction history file\n",
    "history_path = os.path.join('data', 'mnemosyne_history.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# serialize history\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore basic stats about filtered, formatted interaction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load history from file\n",
    "with open(history_path, 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = history.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"Number of interactions = %d\" % len(df)\n",
    "print \"Number of unique students: %d\" % len(df['student_id'].unique())\n",
    "print \"Number of unique assessments: %d\" % history.num_assessments()\n",
    "value_counts = df['outcome'].value_counts()\n",
    "num_passes = value_counts.get(True, 0)\n",
    "num_fails = value_counts.get(False, 0)\n",
    "print \"Overall pass rate: %f\" % (num_passes / (num_passes + num_fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = []\n",
    "for _, g in df.groupby(['user_id', 'module_id']):\n",
    "    ts = g['timestamp'].values\n",
    "    v.extend([nt-t for t, nt in zip(ts[:-1], ts[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Time between reviews (log10-seconds)')\n",
    "plt.ylabel('Frequency (number of reviews)')\n",
    "plt.hist(np.log10(v+1), bins=20)\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'time-between-reviews.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(['user_id', 'module_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = [x for x, g in grouped if len(g) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = grouped.get_group(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts = g['timestamp'].values\n",
    "intervals = [y-x for x, y in zip(ts[:-1], ts[1:])]\n",
    "\n",
    "plt.xlabel('Number of reviews')\n",
    "plt.ylabel('Time until next review (seconds)')\n",
    "plt.title('Review intervals for a single user-item pair')\n",
    "\n",
    "outcomes = g['outcome'].values\n",
    "outcomes = outcomes[:-1]\n",
    "plt.bar(range(len(outcomes)), [max(intervals)] * len(outcomes), width=1, color=['green' if x else 'red' for x in outcomes], alpha=0.25, linewidth=0.)\n",
    "\n",
    "plt.step(range(len(intervals)+1), intervals+[intervals[-1]], where='post')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlim([0, len(intervals)])\n",
    "plt.ylim([0, max(intervals)])\n",
    "\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'review-history-example.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = df['user_id'].value_counts().values\n",
    "plt.xlabel('Number of interactions per student')\n",
    "plt.ylabel('Frequency (number of students)')\n",
    "plt.hist(counts)\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'num_ixns_per_student.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = df['module_id'][df['module_type'] == datatools.AssessmentInteraction.MODULETYPE].value_counts().values\n",
    "\n",
    "plt.xlabel('Number of interactions per item')\n",
    "plt.ylabel('Frequency (number of items)')\n",
    "plt.hist(counts)\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'num_ixns_per_item.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = df.groupby(['user_id', 'module_id']).size().values\n",
    "\n",
    "plt.xlabel('Number of interactions per student per item')\n",
    "plt.ylabel('Frequency (number of student-item pairs)')\n",
    "plt.hist(counts)\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'num_ixns_per_student_per_item.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_students_per_module = [len(group['user_id'].unique()) for _, group in df.groupby('module_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Number of students per item')\n",
    "plt.ylabel('Frequency (number of items)')\n",
    "plt.hist(num_students_per_module)\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'num-students-per-item.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pass_rates(grouped):\n",
    "    \"\"\"\n",
    "    Get pass rate for each group\n",
    "    \n",
    "    :param pd.GroupBy grouped: A grouped dataframe\n",
    "    :rtype: dict[str, float]\n",
    "    :return: A dictionary mapping group name to pass rate\n",
    "    \"\"\"\n",
    "    pass_rates = {}\n",
    "    for name, group in grouped:\n",
    "        vc = group['outcome'].value_counts()\n",
    "        if True not in vc:\n",
    "            pass_rates[name] = 0\n",
    "        else:\n",
    "            pass_rates[name] = vc[True] / len(group)\n",
    "    return pass_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df[df['module_type']==datatools.AssessmentInteraction.MODULETYPE].groupby('user_id')\n",
    "\n",
    "plt.xlabel('Student pass rate')\n",
    "plt.ylabel('Frequency (number of students)')\n",
    "plt.hist(get_pass_rates(grouped).values())\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'student-pass-rates.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df[df['module_type']==datatools.AssessmentInteraction.MODULETYPE].groupby('module_id')\n",
    "\n",
    "plt.xlabel('Assessment pass rate')\n",
    "plt.ylabel('Frequency (number of assessments)')\n",
    "plt.hist(get_pass_rates(grouped).values())\n",
    "plt.yscale('log')\n",
    "plt.savefig(os.path.join('figures', 'mnemosyne', 'assessment-pass-rates.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_flow_graph(interaction_logs):\n",
    "    \"\"\"\n",
    "    Create a graphviz object for the graph of \n",
    "    module transitions across all student paths\n",
    "    \n",
    "    :param pd.DataFrame interaction_logs: An interaction history\n",
    "    :rtype pgv.AGraph\n",
    "    :return Graph of module transitions in student paths\n",
    "    \"\"\"\n",
    "    G = pgv.AGraph(directed=True)\n",
    "\n",
    "    for module_id in interaction_logs['module_id'].unique():\n",
    "        G.add_node(module_id)\n",
    "\n",
    "    E = defaultdict(set)\n",
    "    grouped = interaction_logs.groupby('user_id')\n",
    "    for student_id, group in grouped:\n",
    "        module_ids_in_student_path = group['module_id']\n",
    "        for source_node, target_node in zip(module_ids_in_student_path[:-1], module_ids_in_student_path[1:]):\n",
    "            if source_node != target_node: # stationary\n",
    "                E[(source_node, target_node)] |= {student_id}\n",
    "\n",
    "    for (source_node, target_node), students_that_made_transition in E.iteritems():\n",
    "        G.add_edge(\n",
    "            source_node,\n",
    "            target_node,\n",
    "            weight=len(students_that_made_transition))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = make_flow_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G.write(os.path.join('figures', 'mnemosyne', 'mnemosyne_flow_graph.dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
